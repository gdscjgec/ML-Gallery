# -*- coding: utf-8 -*-
"""Wine-quality1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xPD_6LoknFu5BuCx7CdCSSlJrMAxtefi

<img src="https://c1.staticflickr.com/5/4124/5096953439_5a41df8055_b.jpg"  align=center>
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

data=pd.read_csv('/kaggle/input/cusersmarildownloadswinecsv//wine.csv',sep=";") 
#Loading Data from kaggle

"""## Basic Data Summary"""

data.head()

data.info(verbose=True)

data['alcohol'] = pd.to_numeric(data['alcohol'],errors='coerce')

data.describe()

data['alcohol'].fillna(data['alcohol'].mean(), inplace = True)

"""## Data Visualization"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""### Boxplots for different feature variables across different categories of Quality"""

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "fixed_acidity",palette='rocket' ,ax=ax, data=data)  #Boxplot for Quality and Fixed Acidity

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "volatile_acidity",palette='rocket' , ax=ax, data=data)  #Boxplot for Quality and Volatile Acidity

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "citric_acid",palette='rocket' , ax=ax, data=data)     #Boxplot for Quality and Citric Acid

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "residual_sugar",palette='rocket' , ax=ax, data=data)   #Boxplot for Quality and Residual Sugar

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "chlorides",palette='rocket' , ax=ax, data=data)   #Boxplot for Quality and Chlorides

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "free_sulfur_dioxide",palette='rocket' , ax=ax, data=data)     #Boxplot for Quality and free sulfur dioxide

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "total_sulfur_dioxide",palette='rocket' , ax=ax, data=data)  #Boxplot for Quality and Total sulfur Dioxide

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "pH",palette='rocket' , ax=ax, data=data)  #Boxplot for Quality and pH

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "sulphates",palette='rocket' , ax=ax, data=data)   #Boxplot for Quality and Sulphates

fig, ax = plt.subplots(figsize=(10,8))
sns.boxplot(x = "quality", y = "alcohol",palette='rocket' , ax=ax, data=data)  #Boxplot for Quality and Alcohol

"""### Scatterplots for different feature variables """

fig, ax = plt.subplots(figsize=(10,8))
sns.scatterplot(x = "volatile_acidity", y = "fixed_acidity",palette='rocket' , ax=ax, data=data)
#Scatterplot for Volatilit acidity and Fixed Acidity

fig, ax = plt.subplots(figsize=(10,8))
sns.scatterplot(x = "citric_acid", y = "residual_sugar",palette='rocket' , ax=ax, data=data)
#Scatterplot for Citric Acid vs Residual Sugar

fig, ax = plt.subplots(figsize=(10,8))
sns.scatterplot(x = "free_sulfur_dioxide", y = "total_sulfur_dioxide",palette='rocket' , ax=ax, data=data)
#Scatterplot for Total sulfur dioxide vs Free sulfur dioxide

fig, ax = plt.subplots(figsize=(10,8))
sns.scatterplot(x = "chlorides", y = "sulphates",palette='rocket' , ax=ax, data=data)
#Scatterplot for Chlorides vs Sulphates

fig, ax = plt.subplots(figsize=(10,8))
sns.scatterplot(x = "alcohol", y = "pH",palette='red' , ax=ax, data=data)
#Scatterplot for alcohol vs pH

"""### Correlation across different features"""

plt.subplots(figsize=(16,12))
ax = plt.axes()
ax.set_title("Wine Characteristic Correlation Heatmap")
corr = data.corr()
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values,
           cmap="Reds")                       #Correlation Map for all the features
plt.show()

"""## Data cleaning and prepocessing"""

df=data

"""### Removing the outliers detected with the help of the Boxplots !"""

lower_limit = df["free_sulfur_dioxide"].mean() - 3*df["free_sulfur_dioxide"].std()
upper_limit = df["free_sulfur_dioxide"].mean() + 3*df["free_sulfur_dioxide"].std()

df2 = df[(df["free_sulfur_dioxide"] > lower_limit) & (df["free_sulfur_dioxide"] < upper_limit)]

lower_limit = df2['total_sulfur_dioxide'].mean() - 3*df2['total_sulfur_dioxide'].std()
upper_limit = df2['total_sulfur_dioxide'].mean() + 3*df2['total_sulfur_dioxide'].std()

df3 = df2[(df2['total_sulfur_dioxide'] > lower_limit) & (df2['total_sulfur_dioxide'] < upper_limit)]

lower_limit = df3['residual_sugar'].mean() - 3*df3['residual_sugar'].std()
upper_limit = df3['residual_sugar'].mean() + 3*df3['residual_sugar'].std()

df4 = df3[(df3['residual_sugar'] > lower_limit) & (df3['residual_sugar'] < upper_limit)]

lower_limit = df4["chlorides"].mean() - 3*df4["chlorides"].std()
upper_limit = df4["chlorides"].mean() + 3*df4["chlorides"].std()

df5 = df4[(df4['chlorides'] > lower_limit) & (df4['chlorides'] < upper_limit)]

lower_limit = df5["sulphates"].mean() - 3*df5["sulphates"].std()
upper_limit = df5["sulphates"].mean() + 3*df5["sulphates"].std()

df6 = df5[(df5['sulphates'] > lower_limit) & (df5['sulphates'] < upper_limit)]

lower_limit = df6["volatile_acidity"].mean() - 3*df6["volatile_acidity"].std()
upper_limit = df6["volatile_acidity"].mean() + 3*df6["volatile_acidity"].std()

df7 = df6[(df6['volatile_acidity'] > lower_limit) & (df6['volatile_acidity'] < upper_limit)]

lower_limit = df7["fixed_acidity"].mean() - 3*df7["fixed_acidity"].std()
upper_limit = df7["fixed_acidity"].mean() + 3*df7["fixed_acidity"].std()

df8 = df7[(df7['fixed_acidity'] > lower_limit) & (df7['fixed_acidity'] < upper_limit)]

lower_limit = df8["citric_acid"].mean() - 3*df8["citric_acid"].std()
upper_limit = df8["citric_acid"].mean() + 3*df8["citric_acid"].std()

df9 = df8[(df8['citric_acid'] > lower_limit) & (df8['citric_acid'] < upper_limit)]

lower_limit = df9["pH"].mean() - 3*df9["pH"].std()
upper_limit = df9["pH"].mean() + 3*df9["pH"].std()

df10 = df9[(df9['pH'] > lower_limit) & (df9['pH'] < upper_limit)]

"""### Quality mapping as `3,4` ->`Low`, `5, 6, 7` -> `Medium`, `8, 9` ->`high` """

quality_mapping = { 3 : "Low", 4 : "Low", 5: "Medium",6 : "Medium",7: "Medium",8 : "High",9 : "High"}
df10["quality"] =  df10["quality"].map(quality_mapping)

"""### Categorical to numeric mapping"""

quality_code = {"Low" : 0,"Medium": 1,"High" : 2}
df10["quality"] =  df10["quality"].map(quality_code)

train=df10

"""## Data Split"""

from sklearn.model_selection import train_test_split

X=train.drop(['quality'],axis=1)   # separating the independent variables and dependent variable
y=train['quality']

"""## Feature Scaling

### Using Standard Scalar to scale down features variables
"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)
print(X.shape)

"""### Dimesionality Reduction by PCA"""

from sklearn.decomposition import PCA
pca = PCA()
x_pca = pca.fit_transform(X)

plt.figure(figsize=(10,10))
plt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')
plt.grid()

pca_new = PCA(n_components=10)
x_new = pca_new.fit_transform(X)

print(x_new.shape)
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size = 0.2,random_state=1)

"""## Model Development"""

from sklearn.model_selection import GridSearchCV    # for getting the best hyperparameters
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

# importing and initializing different classifier to use as base_estimator for adaboost
from sklearn.svm import SVC
svc=SVC(probability=True, kernel='linear')
from sklearn.ensemble import ExtraTreesClassifier
etc = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)
rfc = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)

model_params  = {
    "svm" : {
        "model":SVC(gamma="auto"),
        "params":{
            'C' : [1,10,20],
            'kernel':["rbf"]
        }
    },
    
    "decision_tree":{
        "model": DecisionTreeClassifier(random_state=0),
        "params":{
            'criterion':["entropy","gini"],
            "max_depth":[5,8,9]
        }
    },
    
    "random_forest":{
        "model": RandomForestClassifier(random_state=0),
        "params":{
            "n_estimators":[1,5,10],
            "max_depth":[5,8,9,10]
        }
    },
    "naive_bayes":{
        "model": GaussianNB(),
        "params":{}
    },
    
    'logistic_regression' : {
        'model' : LogisticRegression(solver='liblinear',multi_class = 'auto'),
        'params': {
            "C" : [1,5,10]
        }
    },
    
    "adaboost":{
        "model": AdaBoostClassifier(random_state=0),
        "params":{
            "base_estimator": [svc, etc, rfc],
            "n_estimators":[5,10, 15],
            "learning_rate":[0.5,1.0,0.8, 0.1]
        }
    },
    
    "gradient_boosting":{
        "model": GradientBoostingClassifier(random_state=0),
        "params":{
            "max_depth": [5, 7, 10],
            "n_estimators":[5,10, 15],
            "learning_rate":[0.5,1.0,0.8, 0.1]
        }
    }
    
}

score=[]
for model_name,mp in model_params.items():
    clf = GridSearchCV(mp["model"],mp["params"],cv=4,return_train_score=False)
    clf.fit(X_train,y_train)
    score.append({
        "Model" : model_name,
        "Best_Score": clf.best_score_,
        "Best_Params": clf.best_params_
    })

result = pd.DataFrame(score,columns=["Model","Best_Score","Best_Params"])

result

best_ada_params = result.loc[5, ['Best_Params']][0]

best_ada_params   # best parameters for adaboost

from sklearn.model_selection import cross_val_score    # cross validating 
clf_rf = RandomForestClassifier(max_depth= 10, n_estimators=10, random_state=1)
scores = cross_val_score(clf_rf,X_train,y_train,cv=8,scoring="accuracy")

scores.mean()*100   # average score by random forest

clf_rf.fit(X_train,y_train)
y_pred = clf_rf.predict(X_test)

from sklearn.metrics import accuracy_score  # accuracy score by random forest
acc = accuracy_score(y_test,y_pred)       
print(acc*100)

clf_ada = AdaBoostClassifier(base_estimator = etc, learning_rate=0.5, n_estimators = 5, random_state=1)
scores_ada = cross_val_score(clf_ada,X_train,y_train,cv=5,scoring="accuracy")

scores_ada.mean()*100    # average score by adaboost

clf_ada.fit(X_train,y_train)
ada_y_pred = clf_ada.predict(X_test)

ada_acc = accuracy_score(y_test,ada_y_pred)  # accuracy score by adaboost
print(ada_acc*100)